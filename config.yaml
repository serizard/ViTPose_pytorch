# Train config ---------------------------------------
log_level: logging.INFO
seed: 0
deterministic: True # whether not to evaluate the checkpoint during training
cudnn_benchmark: True # Use cudnn 
resume_from: False # CKPT path
gpu_ids: [0]
launcher: 'none' # When distributed training ['none', 'pytorch', 'slurm', 'mpi']
use_amp: True
validate: True
save_interval: 10

autoscale_lr: True # automatically scale lr with the number of gpus

dist_params:
  ...
